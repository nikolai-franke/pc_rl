defaults:
  - algo: ppo
  - model: ppo_continuous
  - env: reach
  - optimizer: ppo_basic
  - _self_

parallel: True
batch_T: 128
batch_B: 8
reward_clip_min: -5
reward_clip_max: 5
max_steps_decorrelate: 0
device: null
runner:
  n_steps: 10_000_000
  log_interval_steps: 1024
  eval_interval_steps: 10240
eval:
  n_eval_envs: 1
  max_traj_length: 500
  max_trajectories: 1

use_slurm: False
video_path: videos
