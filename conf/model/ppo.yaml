defaults:
  - attention: default
  - pos_embedder : default
mask_ratio: 0.0
transformer_block:
  mlp_layers: [512, 1536, 512]
  act: gelu
  dropout: 0.1
encoder_depth: 12
decoder_depth: 6
rl_model:
  act: relu
  mlp_layers: [512, 512]
  init_log_std: 0.0
