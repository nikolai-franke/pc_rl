defaults:
  - _self_
  - algo: ppo
  - model: ppo_continuous
  - env: reach_continuous

parallel: True
batch_T: 128
batch_B: 8
reward_clip_min: -5
reward_clip_max: 5
max_steps_decorrelate: 0
device: null

runner:
  n_steps: 8388608
  log_interval_steps: 4096
  eval_interval_steps: 20480

eval:
  n_eval_envs: 4
  max_traj_length: 100
  min_trajectories: 4
  deterministic_actions: False

env:
  action_type: continuous
